import os
import glob
from pathlib import Path
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.embeddings import Embeddings  # ‚úÖ –î–û–ë–ê–í–õ–ï–ù–û
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer
import shutil

# Unstructured (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.partition.pdf import partition_pdf
    UNSTRUCTURED_AVAILABLE = True
    print("‚úÖ Unstructured –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    UNSTRUCTURED_AVAILABLE = False
    print("‚ö†Ô∏è  –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install 'unstructured[docx,pdf]' –¥–ª—è DOCX/PDF")

class LocalEmbeddings(Embeddings):  # ‚úÖ –ù–ê–°–õ–ï–î–£–ï–ú Embeddings!
    def __init__(self, model_name="paraphrase-multilingual-mpnet-base-v2"):
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (multilingual –¥–ª—è RU)...")
        self.model = SentenceTransformer(model_name)
        print("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≥–æ—Ç–æ–≤—ã")
    
    def embed_documents(self, texts):
        return self.model.encode(texts).tolist()
    
    def embed_query(self, text):
        return self.model.encode([text])[0].tolist()

class RobustDocumentLoader:
    @staticmethod
    def load_txt(file_path: str) -> List[Document]:
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
                if content:
                    return [Document(page_content=content, metadata={"source": file_path})]
        except Exception as e:
            print(f"‚ö†Ô∏è  TXT {file_path}: {e}")
        return []
    
    @staticmethod
    def load_docx(file_path: str) -> List[Document]:
        # –ò–≥–Ω–æ—Ä –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        if file_path.endswith('~$'):
            return []
            
        if UNSTRUCTURED_AVAILABLE:
            try:
                elements = partition_docx(file_path)
                text = '\n'.join([el.text for el in elements if el.text and el.text.strip()])
                if text.strip():
                    return [Document(page_content=text.strip(), metadata={"source": file_path})]
            except Exception as e:
                print(f"‚ö†Ô∏è  Unstructured DOCX {file_path}: {e}")
        
        # Fallback python-docx ‚úÖ –§–ò–ö–°
        try:
            from docx import Document
            doc = Document(file_path)
            text = '\n'.join([para.text for para in doc.paragraphs if para.text.strip()])
            if text.strip():
                return [Document(page_content=text.strip(), metadata={"source": file_path})]
        except Exception as e:
            print(f"‚ö†Ô∏è  DOCX {file_path}: {e}")
        return []
    
    @staticmethod
    def load_pdf(file_path: str) -> List[Document]:
        if UNSTRUCTURED_AVAILABLE:
            try:
                elements = partition_pdf(file_path)
                text = '\n'.join([el.text for el in elements if el.text and el.text.strip()])
                if text.strip():
                    return [Document(page_content=text.strip(), metadata={"source": file_path})]
            except Exception as e:
                print(f"‚ö†Ô∏è  Unstructured PDF {file_path}: {e}")
        
        # Fallback PyPDF2
        try:
            import PyPDF2
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            if text.strip():
                return [Document(page_content=text.strip(), metadata={"source": file_path})]
        except Exception as e:
            print(f"‚ö†Ô∏è  PDF {file_path}: {e}")
        return []

def load_knowledge_base(folder: str = "./knowledge_base/") -> List[Document]:
    if not Path(folder).exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ {folder} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return []
        
    print("üìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ knowledge_base/")
    all_docs = []
    
    # TXT
    for file in Path(folder).rglob("*.txt"):
        docs = RobustDocumentLoader.load_txt(str(file))
        all_docs.extend(docs)
        if docs:
            print(f"üìÑ TXT: {file.name}")
    
    # DOCX
    for file in Path(folder).rglob("*.docx"):
        docs = RobustDocumentLoader.load_docx(str(file))
        all_docs.extend(docs)
        if docs:
            print(f"üìÑ DOCX: {file.name}")
    
    # PDF
    for file in Path(folder).rglob("*.pdf"):
        docs = RobustDocumentLoader.load_pdf(str(file))
        all_docs.extend(docs)
        if docs:
            print(f"üìÑ PDF: {file.name}")
    
    print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return all_docs

class RAGPro:
    def __init__(self):
        with open("api_key.txt", "r", encoding='utf-8') as f:
            self.api_key = f.read().strip()
        
        self.embeddings = LocalEmbeddings()
        self.llm = ChatOpenAI(model="gpt-4o-mini", api_key=self.api_key, temperature=0)
        self.vectorstore = None
        self.chain = None
        
    def build_index(self, docs: List[Document]):
        if os.path.exists("faiss_index"):
            shutil.rmtree("faiss_index")
            print("üóëÔ∏è  –£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π faiss_index")
        
        if not docs:
            print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞!")
            return None
            
        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
        splits = splitter.split_documents(docs)
        print(f"‚úÇÔ∏è  –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(splits)} —á–∞–Ω–∫–æ–≤")
        
        self.vectorstore = FAISS.from_documents(splits, self.embeddings)
        self.vectorstore.save_local("faiss_index")
        
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 20})
        
        template = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ö–û–ù–¢–ï–ö–°–¢–ê.

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ, —Ü–∏—Ç–∏—Ä—É—è –∏—Å—Ç–æ—á–Ω–∏–∫ (—Ñ–∞–π–ª).
2. –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö ‚Üí "–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π".
3. –†—É—Å—Å–∫–∏–π —è–∑—ã–∫, –∫—Ä–∞—Ç–∫–æ.

–ö–û–ù–¢–ï–ö–°–¢:
{context}

–í–û–ü–†–û–°: {question}

–û–¢–í–ï–¢:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        def format_context(docs):
            return "\n\n".join(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n{doc.page_content}" for doc in docs)
        
        chain = (
            {"context": retriever | format_context, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        self.chain = chain
        
        print(f"‚úÖ FAISS –≥–æ—Ç–æ–≤ (k=20, {len(splits)} —á–∞–Ω–∫–æ–≤)")
        return self.vectorstore
    
    def ask(self, question: str) -> str:
        return self.chain.invoke(question)
    
    def extract(self, question: str) -> str:
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 25})
        docs = retriever.invoke(question)
        context = "\n\n".join(doc.page_content for doc in docs)
        prompt = f"""–ò–ó–í–õ–ï–ö–ò —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê. –¶–∏—Ç–∏—Ä—É–π. –ù–µ—Ç ‚Üí "–ù–ï –ù–ê–®–Å–õ".

–ö–û–ù–¢–ï–ö–°–¢: {context}

–í–û–ü–†–û–°: {question}

–û–¢–í–ï–¢:"""
        return self.llm.invoke(prompt).content.strip()

def main():
    print("üöÄ RAG Pro v3.1 - –§–∏–∫—Å Windows/Python 3.14")
    
    docs = load_knowledge_base()
    if not docs:
        print("‚ùå –î–æ–±–∞–≤—å—Ç–µ —Ñ–∞–π–ª—ã –≤ knowledge_base/ –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ")
        return
    
    rag = RAGPro()
    rag.build_index(docs)
    
    print("\nüß™ –¢–ï–°–¢:")
    tests = ["–∫—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä?", "—Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞", "–ø—É–Ω–∫—Ç 2.3"]
    for test in tests:
        answer = rag.ask(test)
        print(f"‚ùì {test}\nüí¨ {answer}\n")
    
    print("ü§ñ –ß–ê–¢ (exit/–≤—ã—Ö–æ–¥ | ! –¥–ª—è extract):")
    while True:
        try:
            question = input("\n‚ùì ").strip()
            if question.lower() in ['exit', '–≤—ã—Ö–æ–¥', 'quit']:
                break
            if question.startswith("!"):
                answer = rag.extract(question[1:])
            else:
                answer = rag.ask(question)
            print(f"üí¨ {answer}")
        except KeyboardInterrupt:
            print("\nüëã")
            break

if __name__ == "__main__":
    main()
